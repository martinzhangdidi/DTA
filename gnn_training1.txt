import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
import pymysql
import json
from torch.utils.tensorboard import SummaryWriter

def create_connection():
    connection = pymysql.connect(
        host='localhost',
        user='root',
        password='zhangdi168',
        database='DTA'
    )
    return connection

class DTADataset(Dataset):
    def __init__(self, data_indices):
        self.data_indices = data_indices

    def __getitem__(self, idx):
        connection = create_connection()
        cursor = connection.cursor(pymysql.cursors.DictCursor)
        cursor.execute("SELECT medGraph, proteinGraph, Affinity FROM kiba_protein_view WHERE id = %s", (self.data_indices[idx],))
        row = cursor.fetchone()
        connection.close()

        if row is None:
            return None

        med_graph = json.loads(row['medGraph'])
        protein_graph = json.loads(row['proteinGraph'])
        affinity = row['Affinity']

        return med_graph, protein_graph, affinity

    def __len__(self):
        return len(self.data_indices)

def read_data_indices():
    connection = create_connection()
    cursor = connection.cursor()
    cursor.execute("SELECT id FROM kiba_protein_view")
    rows = cursor.fetchall()
    data_indices = [row[0] for row in rows]
    connection.close()
    return data_indices

data_indices = read_data_indices()
train_indices, test_indices = train_test_split(data_indices, test_size=1/6, random_state=42)
train_dataset = DTADataset(train_indices)
test_dataset = DTADataset(test_indices)
train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=lambda x: x)
test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, collate_fn=lambda x: x)

class GNNLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super(GNNLayer, self).__init__()
        self.fc = nn.Linear(in_features, out_features)

    def forward(self, x, adj):
        out = torch.matmul(adj, x)
        out = self.fc(out)
        return out

class ConformerBlock(nn.Module):
    def __init__(self, dim, num_heads, ff_mult=4):
        super(ConformerBlock, self).__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * ff_mult),
            nn.ReLU(),
            nn.Linear(dim * ff_mult, dim)
        )

    def forward(self, x):
        attn_output, _ = self.self_attn(x, x, x)
        x = x + attn_output
        x = x + self.ffn(x)
        return x

class GNNModel(nn.Module):
    def __init__(self, num_features, hidden_dim, output_dim):
        super(GNNModel, self).__init__()
        self.med_embedding = nn.Linear(num_features, 128)
        self.protein_embedding = nn.Linear(num_features, 128)
        self.med_gnn1 = GNNLayer(128, hidden_dim)
        self.med_gnn2 = GNNLayer(hidden_dim, hidden_dim)
        self.med_gnn3 = GNNLayer(hidden_dim, hidden_dim)
        self.protein_gnn1 = GNNLayer(128, hidden_dim)
        self.protein_gnn2 = GNNLayer(hidden_dim, hidden_dim)
        self.protein_gnn3 = GNNLayer(hidden_dim, hidden_dim)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.conformer_blocks = nn.ModuleList([ConformerBlock(hidden_dim, num_heads=4) for _ in range(3)])
        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4)
        self.fc1 = nn.Linear(hidden_dim * 2, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 1)

    def forward(self, med_graph, protein_graphs):
        med_x, med_adj = med_graph['atoms'], med_graph['adjacency_matrix']
        med_x = torch.tensor(med_x, dtype=torch.float32)
        med_adj = torch.tensor(med_adj, dtype=torch.float32)
        med_x = self.med_embedding(med_x)
        med_x = self.med_gnn1(med_x, med_adj)
        med_x = self.med_gnn2(med_x, med_adj)
        med_x = self.med_gnn3(med_x, med_adj)
        med_x = med_x.mean(dim=0)

        protein_embeddings = []
        for protein_graph in protein_graphs:
            protein_x, protein_adj = protein_graph['atoms'], protein_graph['adjacency_matrix']
            protein_x = torch.tensor(protein_x, dtype=torch.float32)
            protein_adj = torch.tensor(protein_adj, dtype=torch.float32)
            protein_x = self.protein_embedding(protein_x)
            protein_x = self.protein_gnn1(protein_x, protein_adj)
            protein_x = self.protein_gnn2(protein_x, protein_adj)
            protein_x = self.protein_gnn3(protein_x, protein_adj)
            protein_x = protein_x.mean(dim=0)
            for conformer in self.conformer_blocks:
                protein_x = conformer(protein_x.unsqueeze(1)).squeeze(1)
            protein_embeddings.append(protein_x)

        protein_embeddings = torch.stack(protein_embeddings, dim=0)
        attn_output, _ = self.attention(protein_embeddings, protein_embeddings, protein_embeddings)
        attn_output = attn_output.mean(dim=0)
        combined_x = torch.cat([med_x, attn_output], dim=0)
        x = self.fc1(combined_x)
        x = self.fc2(x)
        x = self.fc3(x)
        return x

device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
model = GNNModel(num_features=5, hidden_dim=128, output_dim=1).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)
writer = SummaryWriter('runs/gnn_experiment')

def save_model(model, path):
    torch.save(model.state_dict(), path)

def train(model, train_loader, test_loader, criterion, optimizer, num_epochs=1000):
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for batch in train_loader:
            optimizer.zero_grad()
            batch_loss = 0.0
            for data in batch:
                if data is None:
                    continue
                med_graph, protein_graph, affinity = data
                med_graph = {k: torch.tensor(v).to(device) for k, v in med_graph.items()}
                protein_graph = [{k: torch.tensor(v).to(device) for k, v in graph.items()} for graph in protein_graph]
                outputs = model(med_graph, protein_graph)
                loss = criterion(outputs, torch.tensor([affinity], dtype=torch.float32).to(device))
                loss.backward()
                optimizer.step()
                batch_loss += loss.item()
            train_loss += batch_loss
        train_loss /= len(train_loader)
        writer.add_scalar('Loss/train', train_loss, epoch)

        model.eval()
        test_loss = 0.0
        with torch.no_grad():
            for batch in test_loader:
                batch_loss = 0.0
                for data in batch:
                    if data is None:
                        continue
                    med_graph, protein_graph, affinity = data
                    med_graph = {k: torch.tensor(v).to(device) for k, v in med_graph.items()}
                    protein_graph = [{k: torch.tensor(v).to(device) for k, v in graph.items()} for graph in protein_graph]
                    outputs = model(med_graph, protein_graph)
                    loss = criterion(outputs, torch.tensor([affinity], dtype=torch.float32).to(device))
                    batch_loss += loss.item()
                test_loss += batch_loss
        test_loss /= len(test_loader)
        writer.add_scalar('Loss/test', test_loss, epoch)

        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')
        save_model(model, f'model_epoch_{epoch+1}.pth')

    writer.close()

train(model, train_loader, test_loader, criterion, optimizer)