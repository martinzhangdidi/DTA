import pymysql
import pickle
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from torch.utils.tensorboard import SummaryWriter
from torch_geometric.nn import GCNConv, global_mean_pool


# 创建数据库连接
def create_connection():
    return pymysql.connect(
        host='localhost',
        user='root',
        password='zhangdi168',
        database='DTA'
    )


# 自定义数据集类，批量加载和预处理数据
class DTADataset(Dataset):
    def __init__(self, data_indices):
        self.data_indices = data_indices
        self.med_graphs = []
        self.protein_protein_graphs = []
        self.affinities = []
        self.load_data()

    def load_data(self):
        self.med_graphs = []
        self.protein_protein_graphs = []
        self.affinities = []

        connection = create_connection()
        cursor = connection.cursor(pymysql.cursors.DictCursor)

        # 分批次加载数据
        for i in range(0, len(self.data_indices), 400):
            batch_indices = self.data_indices[i:i + 400]
            cursor.execute("SELECT id, med_blob, protein_blob, Affinity FROM kiba_view WHERE id IN (%s)" % ",".join(map(str, batch_indices)))
            rows = cursor.fetchall()
            device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
            for row in rows:
                med_graph = pickle.loads(row['med_blob'])
                protein_protein_graph = pickle.loads(row['protein_blob'])
                affinity = row['Affinity']

                med_graph['atoms'] = torch.tensor(med_graph['atoms'], dtype=torch.float32).to(device)
                med_graph['adjacency_matrix'] = torch.tensor(med_graph['adjacency_matrix'], dtype=torch.float32).to(device)
                for graph in protein_protein_graph:
                    graph['atoms'] = torch.tensor(graph['atoms'], dtype=torch.float32).to(device)
                    graph['adjacency_matrix'] = torch.tensor(graph['adjacency_matrix'], dtype=torch.float32).to(device)
                affinity = torch.tensor([affinity], dtype=torch.float32).to(device)

                self.med_graphs.append(med_graph)
                self.protein_protein_graphs.append(protein_protein_graph)
                self.affinities.append(affinity)
        connection.close()

    def __getitem__(self, idx):
        return self.med_graphs[idx], self.protein_protein_graphs[idx], self.affinities[idx]

    def __len__(self):
        return len(self.affinities)


# 读取数据索引
def read_data_indices():
    connection = create_connection()
    cursor = connection.cursor()
    cursor.execute("SELECT id FROM kiba_view")
    rows = cursor.fetchall()
    connection.close()
    return [row[0] for row in rows]


# 数据预处理
data_indices = read_data_indices()
train_indices, test_indices = train_test_split(data_indices, test_size=1/6, random_state=42)
train_dataset = DTADataset(train_indices)
test_dataset = DTADataset(test_indices)
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)


class GNNLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super(GNNLayer, self).__init__()
        self.fc = nn.Linear(in_features, out_features)

    def forward(self, x, adj):
        out = torch.matmul(adj, x)
        out = self.fc(out)
        return out


class ConformerBlock(nn.Module):
    def __init__(self, dim, num_heads, ff_mult=4):
        super(ConformerBlock, self).__init__()
        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(dim, dim * ff_mult),
            nn.ReLU(),
            nn.Linear(dim * ff_mult, dim)
        )

    def forward(self, x):
        attn_output, _ = self.self_attn(x, x, x)
        x = x + attn_output
        x = x + self.ffn(x)
        return x


class GNNModel(nn.Module):
    def __init__(self, num_features, hidden_dim, output_dim):
        super(GNNModel, self).__init__()
        self.med_embedding = nn.Linear(num_features, 128)
        self.protein_embedding = nn.Linear(num_features, 128)
        self.med_gnn1 = GNNLayer(128, hidden_dim)
        self.med_gnn2 = GNNLayer(hidden_dim, hidden_dim)
        self.med_gnn3 = GNNLayer(hidden_dim, hidden_dim)
        self.protein_gnn1 = GNNLayer(128, hidden_dim)
        self.protein_gnn2 = GNNLayer(hidden_dim, hidden_dim)
        self.protein_gnn3 = GNNLayer(hidden_dim, hidden_dim)
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.conformer_blocks = nn.ModuleList([ConformerBlock(hidden_dim, num_heads=4) for _ in range(3)])
        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4)
        self.fc1 = nn.Linear(hidden_dim * 2, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 1)

    def forward(self, med_graph, protein_graphs):
        med_x, med_adj = med_graph['atoms'], med_graph['adjacency_matrix']
        med_x = self.med_embedding(med_x)
        med_x = self.med_gnn1(med_x, med_adj)
        med_x = self.med_gnn2(med_x, med_adj)
        med_x = self.med_gnn3(med_x, med_adj)
        med_x = med_x.mean(dim=0)

        protein_embeddings = []
        for protein_graph in protein_graphs:
            protein_x, protein_adj = protein_graph['atoms'], protein_graph['adjacency_matrix']
            protein_x = self.protein_embedding(protein_x)
            protein_x = self.protein_gnn1(protein_x, protein_adj)
            protein_x = self.protein_gnn2(protein_x, protein_adj)
            protein_x = self.protein_gnn3(protein_x, protein_adj)
            protein_x = protein_x.mean(dim=0)
            for conformer in self.conformer_blocks:
                protein_x = conformer(protein_x.unsqueeze(1)).squeeze(1)
            protein_embeddings.append(protein_x)

        protein_embeddings = torch.stack(protein_embeddings, dim=0)
        attn_output, _ = self.attention(protein_embeddings, protein_embeddings, protein_embeddings)
        attn_output = attn_output.mean(dim=0)
        combined_x = torch.cat([med_x, attn_output], dim=0)
        x = self.fc1(combined_x)
        x = self.fc2(x)
        x = self.fc3(x)
        return x


device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
model = GNNModel(num_features=5, hidden_dim=128, output_dim=1).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)
writer = SummaryWriter('runs/gnn_experiment')


def save_model(model, path):
    torch.save(model.state_dict(), path)


def train(model, train_loader, test_loader, criterion, optimizer, num_epochs=1000):
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for med_graph, protein_graph, affinity in train_loader:
            optimizer.zero_grad()
            outputs = model(med_graph, protein_graph)
            loss = criterion(outputs, affinity.to(device))
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)
        writer.add_scalar('Loss/train', train_loss, epoch)

        model.eval()
        test_loss = 0.0
        with torch.no_grad():
            for med_graph, protein_graph, affinity in test_loader:
                outputs = model(med_graph, protein_graph)
                loss = criterion(outputs, affinity.to(device))
                test_loss += loss.item()
        test_loss /= len(test_loader)
        writer.add_scalar('Loss/test', test_loss, epoch)

        print(f'Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')
        save_model(model, f'model_epoch_{epoch + 1}.pth')

    writer.close()


train(model, train_loader, test_loader, criterion, optimizer)